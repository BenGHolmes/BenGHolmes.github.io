<div style="background-color: #00193d; padding: 72px;">
    <h1 style="color:white;">Vanessa</h1>
    <h2 style="color: white;">An Autonomous Stuffed Animal Rescue Robot Using Computer Vision</h2>
    <div style="margin-top: 32px;">
        <div class="div-image">
            <img src="Images/realbot.jpg" alt="App Main Screen" style="width: 100%;">
            <video src="Images/best_run.mp4" alt="Video From Competition" style="width: 100%;" controls></video>
        </div>
        <div class="div-text">
            <p>
                During this past summer, I was lucky enough to participate in the yearly Engineering Physics robot design course. The course is structured as a competition, with the goal being to build a fully autonomous robot that is able to complete the given challenge in just 13 weeks. This year, we had to rescue Ewoks (stuffed animals) from the Empire stronghold by navigating over gaps, differentiating between various infrared light signals, and detecting and capturing 5 stuffed Ewoks along the way. Working in a team of 4, my major contributiuon to the robot was the implementation of a computer vision system that would allow us to detect the Ewoks from much farther away than the traditional approach of IR distance sensors.
            </p>

            <p>
                From the beginning, our team decided that we wanted to try to implement a universal solution, that required no hardcoding or prior knowledge of the course. If successful, our robot would be able to navigate any similar style of course, even if the specific obstacles changed. In order to achieve this, we used computer vision. A Raspberry Pi on the robot continuously captured images of the course ahead using a fisheye camera, and these photos were processed using a Python implementation of the extremely fast <a class="link" href="https://pjreddie.com/darknet/yolo/" target="_blank">YOLO</a> network, running on a Raspberry Pi. 
            </p>

            <p>
                While navigating the course, an STM microcontroller would periodically request images from the Raspberry Pi over serial. The neural network would then find the coordinates of the bounding boxes around each Ewok in the frame, and would calculate the heading error by finding the distance between the centre of the frame and the centre of the bounding box. This error was then written to an 8-bit digital to analog converter (a resistor ladder on the PCB) and the analog voltage was read by the STM. This would turn the robot to face the next Ewok, and then the robot would continue driving.
            </p>

            <p>
                The video is of our best run on competition day. Unfortunately, due to a 2:00 time limit for each run, and some failed attempts at the start of our final run, we ran out of time before completing the entire course. For more info about the competition, and Vanessa's mechanical and electrical components, you can have a look at the full website our team built for her <a class="link" href="https://axel-jacobsen.github.io/ENPHRobot/" target="_blank">here</a>.
            </p>

        </div>
    </div>
</div>